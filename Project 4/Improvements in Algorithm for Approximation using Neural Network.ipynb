{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Task: Approximate the $\\sin$ function to the following neural network configuration using various gradient descent algorithms.\n",
    "\n",
    "<center><img src=\"nn_for_sine.png\"></center>\n",
    "\n",
    "This will be done using ordinary gradient descent, conjugate gradient descent, stochastic gradient descent, and stochastic conjugate gradient descent.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Preliminary Work: Import packages and set up general functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "import time\n",
    "import math\n",
    "from sympy import *\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Let $\\mu(x)$ be defined as the output of the neural network where $x$ is the input value: \n",
    "$$ \\mu(x) = \\alpha^0 \\sigma(\\theta_0^0 + \\theta_1^0x) + \\alpha^1 \\sigma(\\theta_0^1 + \\theta_1^1x) + \\alpha^2 \\sigma(\\theta_0^2 + \\theta_1^2x) + \\alpha^3 \\sigma(\\theta_0^3 + \\theta_1^3x) $$ \n",
    "and $\\sigma(x)$ is the sigmoid function: $$\\sigma(x) = \\frac{1}{1 + e^{-x}} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "def mu(x,var_arr):\n",
    "\n",
    "    y = 0\n",
    "    for i in range(4):\n",
    "        y += var_arr[0][i] * sigmoid(x, var_arr[1][i], var_arr[2][i])\n",
    "\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "To aid the multiple arguments in the sigmoid and computation of partial derivative of sigmoid function, let's define two exponential function: $$ \\begin{aligned} \\text{expOne}(x,y,z) &= e^{(y + zx)} \\\\ \\text{expTwo}(x,y,z) &= e^{-(y + zx)} \\end{aligned} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "def expOne(x,y,z):\n",
    "    return (math.exp(y + (z*x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "def expTwo(x,y,z):\n",
    "    return (math.exp(-1 * (y + z*x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Therefore the sigmoid function will be defined as a function of 3 arguments: $$ \\sigma(x,y,z) = \\frac{1}{1 + \\text{expTwo}(x,y,z)} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "def sigmoid(x,y,z):\n",
    "    return (1 + expTwo(x,y,z))**-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The cost function, which minimizes the distance between $\\sin(x)$ and $\\mu(x)$ from a total of $100$ points, is as follows: $$C(\\alpha^0,\\alpha^1,\\alpha^2,\\alpha^3,\\theta_0^0,\\theta_0^1,\\theta_0^2,\\theta_0^3,\\theta_1^0,\\theta_1^1,\\theta_1^2,\\theta_1^3,\\theta_2^0,\\theta_2^1,\\theta_2^2,\\theta_2^3) = \\frac{1}{2}\\sum_{x = 0}^{99} (\\mu(x) - \\sin(x))^2 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "def cost(var_arr):\n",
    "\n",
    "    output = 0.0\n",
    "    interval = np.linspace(0,2*np.pi, 100)\n",
    "\n",
    "    for i in interval:\n",
    "        output += (mu(i,var_arr) - np.sin(i))**2\n",
    "\n",
    "    return 0.5*output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "In the stochastic cost function, only the cost at certain randomized points will be summed together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "def stocCost(var_arr,xvalues):\n",
    "\n",
    "    output = 0.0\n",
    "\n",
    "    for i in xvalues:\n",
    "        output += (mu(i,var_arr) - np.sin(i))**2\n",
    "\n",
    "    return 0.5*output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The partial derivatives are defined as follows: \n",
    "$$ \\begin{aligned} \n",
    "\\frac{\\partial C}{\\partial \\alpha^0} = (\\mu(x) - \\sin(x))\\sigma(\\theta_0^0 + \\theta_1^0x) ~~~\n",
    "\\frac{\\partial C}{\\partial \\theta_0^0} &= \\frac{\\alpha^0(\\mu(x) - \\sin(x))e^{\\theta_0^0 + x\\theta_1^0}}{(1 + e^{\\theta_0^0 + x\\theta_1^0})^2} ~~~\n",
    "\\frac{\\partial C}{\\partial \\theta_1^0} = \\frac{x\\alpha^0(\\mu(x) - \\sin(x))e^{\\theta_0^0 + x\\theta_1^0}}{(1 + e^{\\theta_0^0 + x\\theta_1^0})^2} \\\\\n",
    "\\frac{\\partial C}{\\partial \\alpha^1} = (\\mu(x) - \\sin(x))\\sigma(\\theta_0^1 + \\theta_1^1x) ~~~\n",
    "\\frac{\\partial C}{\\partial \\theta_0^1} &= \\frac{\\alpha^0(\\mu(x) - \\sin(x))e^{\\theta_0^1 + x\\theta_1^1}}{(1 + e^{\\theta_0^1 + x\\theta_1^1})^2} ~~~\n",
    "\\frac{\\partial C}{\\partial \\theta_1^1} = \\frac{x\\alpha^0(\\mu(x) - \\sin(x))e^{\\theta_0^1 + x\\theta_1^1}}{(1 + e^{\\theta_0^1 + x\\theta_1^1})^2} \\\\\n",
    "\\frac{\\partial C}{\\partial \\alpha^2} = (\\mu(x) - \\sin(x))\\sigma(\\theta_0^2 + \\theta_1^2x) ~~~\n",
    "\\frac{\\partial C}{\\partial \\theta_0^2} &= \\frac{\\alpha^0(\\mu(x) - \\sin(x))e^{\\theta_0^2 + x\\theta_1^2}}{(1 + e^{\\theta_0^2 + x\\theta_1^2})^2} ~~~\n",
    "\\frac{\\partial C}{\\partial \\theta_1^2} = \\frac{x\\alpha^0(\\mu(x) - \\sin(x))e^{\\theta_0^2 + x\\theta_1^2}}{(1 + e^{\\theta_0^2 + x\\theta_1^2})^2} \\\\\n",
    "\\frac{\\partial C}{\\partial \\alpha^3} = (\\mu(x) - \\sin(x))\\sigma(\\theta_0^3 + \\theta_1^3x) ~~~\n",
    "\\frac{\\partial C}{\\partial \\theta_0^3} &= \\frac{\\alpha^0(\\mu(x) - \\sin(x))e^{\\theta_0^3 + x\\theta_1^3}}{(1 + e^{\\theta_0^3 + x\\theta_1^3})^2} ~~~\n",
    "\\frac{\\partial C}{\\partial \\theta_1^3} = \\frac{x\\alpha^0(\\mu(x) - \\sin(x))e^{\\theta_0^3 + x\\theta_1^3}}{(1 + e^{\\theta_0^3 + x\\theta_1^3})^2} \\end{aligned} $$ \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "def gradient(var_arr):\n",
    "\n",
    "    grad_arr = np.zeros((3,4))\n",
    "    interval = np.linspace(0,2*np.pi, 100)\n",
    "\n",
    "    for x in interval:\n",
    "        value = mu(x, var_arr) - np.sin(x)\n",
    "        for i in range(4):\n",
    "            grad_arr[0][i] += (value * sigmoid(x,var_arr[1][i],var_arr[2][i]))\n",
    "            temp = expOne(x,var_arr[1][i],var_arr[2][i])\n",
    "            tempTwo = (value * temp * var_arr[0][i]) / ((1 + temp)**2)\n",
    "            grad_arr[1][i] += tempTwo\n",
    "            grad_arr[2][i] += x * tempTwo\n",
    "\n",
    "    return grad_arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "In the stochastic gradient function, only the gradient at certain randomized points will be summed together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "def stocGrad(var_arr,interval):\n",
    "\n",
    "    grad_arr = np.zeros((3,4))\n",
    "\n",
    "    for x in interval:\n",
    "        value = mu(x, var_arr) - np.sin(x)\n",
    "        for i in range(4):\n",
    "            grad_arr[0][i] += (value * sigmoid(x,var_arr[1][i],var_arr[2][i]))\n",
    "            temp = expOne(x,var_arr[1][i],var_arr[2][i])\n",
    "            tempTwo = (value * temp * var_arr[0][i]) / ((1 + temp)**2)\n",
    "            grad_arr[1][i] += tempTwo\n",
    "            grad_arr[2][i] += x * tempTwo\n",
    "\n",
    "    return grad_arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Note: The 12 variables and its gradient will be stored as follows: $$ \\begin{bmatrix} \\alpha^0 & \\alpha^1 & \\alpha^2 & \\alpha^3 \\\\ \\theta_0^0 & \\theta_0^1 & \\theta_0^2 & \\theta_0^3 \\\\ \\theta_1^0 & \\theta_1^1 & \\theta_1^2 & \\theta_1^3 \\end{bmatrix} ~~~ \\begin{bmatrix} \\frac{\\partial C}{\\partial \\alpha^0} & \\frac{\\partial C}{\\partial \\alpha^1} & \\frac{\\partial C}{\\partial \\alpha^2} & \\frac{\\partial C}{\\partial \\alpha^3} \\\\ \\frac{\\partial C}{\\partial \\theta_0^0} & \\frac{\\partial C}{\\partial \\theta_0^1} & \\frac{\\partial C}{\\partial \\theta_0^2} & \\frac{\\partial C}{\\partial \\theta_0^3} \\\\ \\frac{\\partial C}{\\partial \\theta_1^0} & \\frac{\\partial C}{\\partial \\theta_1^1} & \\frac{\\partial C}{\\partial \\theta_1^2} & \\frac{\\partial C}{\\partial \\theta_1^3} \\end{bmatrix} $$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Algorithm $1$: Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "def algorithm1(h,max_steps,tolerance,a_init,printInfo):\n",
    "\n",
    "    print(\"For Algorithm 1 (gradient descent)\")\n",
    "    print(\"Initial Cost: \", cost(a_init))\n",
    "\n",
    "    if(printInfo == 'True'):\n",
    "        print(\"The initial randomized guesses for the constants of the neural network are: \")\n",
    "        for a in range(4):\n",
    "            print(\"alpha_\",a, \" = \", a_init[0][a])\n",
    "        for b in range(4):\n",
    "            print(\"theta_0^\",b, \" = \", a_init[1][b])\n",
    "        for c in range(4):\n",
    "            print(\"theta_1^\",c, \" = \", a_init[2][c])\n",
    "\n",
    "    a_new = a_init - h * gradient(a_init)\n",
    "    steps = 0\n",
    "\n",
    "    for i in range(max_steps):\n",
    "        if(cost(a_init) - cost(a_new) <= tolerance):\n",
    "            print(\"Tolerance has been reached.\")\n",
    "            break\n",
    "        if(cost(a_init) < cost(a_new)):\n",
    "            a_new = a_init\n",
    "            break\n",
    "        a_init = a_new\n",
    "        direction = -1 * gradient(a_init)\n",
    "        a_new = a_init + (h * direction)\n",
    "        steps += 1\n",
    "\n",
    "    print(\"Final Cost: \", cost(a_new))\n",
    "\n",
    "    if(printInfo == 'True'):\n",
    "        print(\"The constants for the neural network are: \")\n",
    "        for j in range(4):\n",
    "            print(\"alpha_\",j, \" = \", a_new[0][j])\n",
    "        for k in range(4):\n",
    "            print(\"theta_0^\",k, \" = \", a_new[1][k])\n",
    "        for l in range(4):\n",
    "            print(\"theta_1^\",l, \" = \", a_new[2][l])\n",
    "\n",
    "    print(steps, \" steps completed.\")\n",
    "\n",
    "    x_0 = np.linspace(0, 2 * np.pi,100)\n",
    "    y_0 = []\n",
    "    y_1 = []\n",
    "    for p in x_0:\n",
    "        y_0.append(mu(p,a_new))\n",
    "        y_1.append(np.sin(p))\n",
    "    fig = plt.figure()\n",
    "    axes = fig.add_axes([0.1, 0.1, 0.9, 0.7])\n",
    "    axes.plot(x_0,y_0,label =\"$\\mu$, approximated using GD\",c = \"red\");\n",
    "    axes.plot(x_0,y_1, label = \"$\\sin(x)$\", c = \"blue\");\n",
    "    axes.legend(loc = 3);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Algorithm 2: Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "def algorithm2(h,max_steps,tolerance,a_init,size,subset,printInfo):\n",
    "\n",
    "\n",
    "    print(\"For Algorithm 2 (stochastic gradient descent)\")\n",
    "    print(\"Initial Cost: \", stocCost(a_init,subset))\n",
    "\n",
    "    if(printInfo == 'True'):\n",
    "        print(\"The initial randomized guesses for the constants of the neural network are: \")\n",
    "        for a in range(4):\n",
    "            print(\"alpha_\",a, \" = \", a_init[0][a])\n",
    "        for b in range(4):\n",
    "            print(\"theta_0^\",b, \" = \", a_init[1][b])\n",
    "        for c in range(4):\n",
    "            print(\"theta_1^\",c, \" = \", a_init[2][c])\n",
    "\n",
    "    a_new = a_init - h * stocGrad(a_init,subset)\n",
    "    steps = 0\n",
    "\n",
    "    for i in range(max_steps):\n",
    "        if(stocCost(a_init,subset) - stocCost(a_new,subset) <= tolerance):\n",
    "            print(\"Tolerance has been reached.\")\n",
    "            break\n",
    "        if(stocCost(a_init,subset) < stocCost(a_new,subset)):\n",
    "            a_new = a_init\n",
    "            break\n",
    "        a_init = a_new\n",
    "        direction = -1 * stocGrad(a_init,subset)\n",
    "        a_new = a_init + (h * direction)\n",
    "        steps += 1\n",
    "\n",
    "    print(\"Final Cost: \", stocCost(a_new,subset))\n",
    "\n",
    "    if(printInfo == 'True'):\n",
    "        print(\"The constants for the neural network are: \")\n",
    "        for j in range(4):\n",
    "            print(\"alpha_\",j, \" = \", a_new[0][j])\n",
    "        for k in range(4):\n",
    "            print(\"theta_0^\",k, \" = \", a_new[1][k])\n",
    "        for l in range(4):\n",
    "            print(\"theta_1^\",l, \" = \", a_new[2][l])\n",
    "\n",
    "    print(steps, \" steps completed using \", size, \" randomized points from 0 to 2pi.\")\n",
    "\n",
    "    x_0 = np.linspace(0, 2 * np.pi,100)\n",
    "    y_0 = []\n",
    "    y_1 = []\n",
    "    for p in x_0:\n",
    "        y_0.append(mu(p,a_new))\n",
    "        y_1.append(np.sin(p))\n",
    "    fig = plt.figure()\n",
    "    axes = fig.add_axes([0.1, 0.1, 0.9, 0.7])\n",
    "    axes.plot(x_0,y_0,label =\"$\\mu$, approximated using SGD\",c = \"red\");\n",
    "    axes.plot(x_0,y_1, label = \"$\\sin(x)$\", c = \"blue\");\n",
    "    axes.legend(loc = 3);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Algorithm 3: Conjugate Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "In conjugate gradient descent,\n",
    "\n",
    "$$ \\min_x = \\frac{\\Big(\\frac{b_2}{2}\\Big) (a_0 + a_1) - b_1(a_0 + a_2) + \\Big(\\frac{b_0}{2}\\Big)(a_1 + a_2)}{b_2 - 2b_1 + b_0} $$\n",
    "\n",
    "where $(a_0,b_0)~,~(a_1,b_2)~,~(a_2,b_2)$ are evenly spaced and go in order from smallest to largest. It represent points on a parabola determined by $$a_1 = a_0 -h \\nabla g(a_0) \\text{  and  } a_2 = a_1 - h \\nabla g(a_0)$$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "def algorithm3(h,max_steps,tolerance,a_init,printInfo):\n",
    "\n",
    "    steps = 0\n",
    "\n",
    "    print(\"For Algorithm 3 (conjugate gradient descent)\")\n",
    "    print(\"Initial Cost: \", cost(a_init))\n",
    "    \n",
    "    if(printInfo == 'True'):\n",
    "        print(\"The initial guesses for the constants of the neural network are: \")\n",
    "        for a in range(4):\n",
    "            print(\"alpha_\",a, \" = \", a_init[0][a])\n",
    "        for b in range(4):\n",
    "            print(\"theta_0^\",b, \" = \", a_init[1][b])\n",
    "        for c in range(4):\n",
    "            print(\"theta_1^\",c, \" = \", a_init[2][c])\n",
    "\n",
    "    for i in range(max_steps):\n",
    "        a_0 = a_init\n",
    "        b_0 = cost(a_init)\n",
    "        a_1 = a_0 - (h * gradient(a_0))\n",
    "        b_1 = cost(a_1)\n",
    "        a_2 = a_1 - (h * gradient(a_0))\n",
    "        b_2 = cost(a_2)\n",
    "        min_a = (0.5*b_2*(a_0 + a_1) - b_1*(a_0 + a_2) + 0.5*b_0*(a_1 + a_2))/(b_2 - 2*b_1 +b_0)\n",
    "        a_init = min_a\n",
    "        steps += 1\n",
    "\n",
    "    print(\"Final Cost: \", cost(a_init))\n",
    "\n",
    "    if(printInfo == 'True'):\n",
    "        print(\"The constants for the neural network are: \")\n",
    "        for j in range(4):\n",
    "            print(\"alpha_\",j, \" = \", a_init[0][j])\n",
    "        for k in range(4):\n",
    "            print(\"theta_0^\",k, \" = \", a_init[1][k])\n",
    "        for l in range(4):\n",
    "            print(\"theta_1^\",l, \" = \", a_init[2][l])\n",
    "\n",
    "    print(steps, \" steps completed.\")\n",
    "\n",
    "    x_0 = np.linspace(0, 2 * np.pi,100)\n",
    "    y_0 = []\n",
    "    y_1 = []\n",
    "    for p in x_0:\n",
    "        y_0.append(mu(p,a_init))\n",
    "        y_1.append(np.sin(p))\n",
    "    fig = plt.figure()\n",
    "    axes = fig.add_axes([0.1, 0.1, 0.9, 0.7])\n",
    "    axes.plot(x_0,y_0,label =\"$\\mu$, approximated using CGD\",c = \"red\");\n",
    "    axes.plot(x_0,y_1, label = \"$\\sin(x)$\", c = \"blue\");\n",
    "    axes.legend(loc = 3);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Algorithm $4$: Conjugate Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "def algorithm4(h,max_steps,tolerance,a_init,size,subset,printInfo):\n",
    "\n",
    "    steps = 0\n",
    "\n",
    "    print(\"For Algorithm 4 (stochastic conjugate gradient descent)\")\n",
    "    print(\"Initial Cost: \", stocCost(a_init,subset))\n",
    "\n",
    "    if(printInfo == 'True'):\n",
    "        print(\"The initial randomized guesses for the constants of the neural network are: \")\n",
    "        for a in range(4):\n",
    "            print(\"alpha_\",a, \" = \", a_init[0][a])\n",
    "        for b in range(4):\n",
    "            print(\"theta_0^\",b, \" = \", a_init[1][b])\n",
    "        for c in range(4):\n",
    "            print(\"theta_1^\",c, \" = \", a_init[2][c])\n",
    "\n",
    "    for i in range(max_steps):\n",
    "        a_0 = a_init\n",
    "        b_0 = stocCost(a_init,subset)\n",
    "        a_1 = a_0 - (h * stocGrad(a_0,subset))\n",
    "        b_1 = stocCost(a_1,subset)\n",
    "        a_2 = a_1 - (h * stocGrad(a_0,subset))\n",
    "        b_2 = stocCost(a_2,subset)\n",
    "        min_a = (0.5*b_2*(a_0 + a_1) - b_1*(a_0 + a_2) + 0.5*b_0*(a_1 + a_2))/(b_2 - 2*b_1 +b_0)\n",
    "        a_init = min_a\n",
    "        steps += 1\n",
    "\n",
    "    print(\"Final Cost: \", stocCost(a_init,subset))\n",
    "\n",
    "    if(printInfo == 'True'):\n",
    "        print(\"The constants for the neural network are: \")\n",
    "        for j in range(4):\n",
    "            print(\"alpha_\",j, \" = \", a_init[0][j])\n",
    "        for k in range(4):\n",
    "            print(\"theta_0^\",k, \" = \", a_init[1][k])\n",
    "        for l in range(4):\n",
    "            print(\"theta_1^\",l, \" = \", a_init[2][l])\n",
    "\n",
    "    print(steps, \" steps completed using \", size, \" randomized points from 0 to 2pi.\")\n",
    "\n",
    "    x_0 = np.linspace(0, 2 * np.pi,100)\n",
    "    y_0 = []\n",
    "    y_1 = []\n",
    "    for p in x_0:\n",
    "        y_0.append(mu(p,a_init))\n",
    "        y_1.append(np.sin(p))\n",
    "\n",
    "    fig = plt.figure()\n",
    "    axes = fig.add_axes([0.1, 0.1, 0.9, 0.7])\n",
    "    axes.plot(x_0,y_0,label =\"$\\mu$, approximated using SCGD\",c = \"red\");\n",
    "    axes.plot(x_0,y_1, label = \"$\\sin(x)$\", c = \"blue\");\n",
    "    axes.legend(loc = 3);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Run all algorithms and see which finishes the fastest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Algorithm 1 (gradient descent)\n",
      "Initial Cost:  254.696759663\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tolerance has been reached.\n",
      "Final Cost:  2.46382798009\n",
      "6102  steps completed.\n",
      "For Algorithm 2 (stochastic gradient descent)\n",
      "Initial Cost:  76.6865395042\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tolerance has been reached.\n",
      "Final Cost:  1.04675873328\n",
      "9053  steps completed using  35  randomized points from 0 to 2pi.\n",
      "For Algorithm 3 (conjugate gradient descent)"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initial Cost:  254.696759663\n",
      "Final Cost:  0.00558544620592\n",
      "10000  steps completed.\n",
      "For Algorithm 4 (stochastic conjugate gradient descent)\n",
      "Initial Cost:  76.6865395042\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Cost:  0.00264554247359\n",
      "10000  steps completed using  35  randomized points from 0 to 2pi.\n",
      "Using gradient descent, the algorithm took: 50.833911418914795  seconds.\n",
      "Using stochastic gradient descent, the algorithm took: 26.66066575050354  seconds.\n",
      "Using conjugate gradient descent, the algorithm took: 100.0600233078003  seconds.\n",
      "Using stochastic conjugate gradient descent, the algorithm took: 32.391128063201904  seconds.\n"
     ]
    },
    {
     "data": {
      "image/png": "664d2bd50fc9d75a64c4ebcd0575998b50f78748"
     },
     "metadata": {
      "image/png": {
       "height": 234,
       "width": 436
      }
     }
    },
    {
     "data": {
      "image/png": "23c681b193d4ad8a243c184507e5833e51713b87"
     },
     "metadata": {
      "image/png": {
       "height": 234,
       "width": 442
      }
     }
    },
    {
     "data": {
      "image/png": "60c77094c236f8b84489aa2ea99b56e1a7c609ef"
     },
     "metadata": {
      "image/png": {
       "height": 234,
       "width": 442
      }
     }
    },
    {
     "data": {
      "image/png": "b2863e851853ac8d4960166aefb4b3ee3beed804"
     },
     "metadata": {
      "image/png": {
       "height": 234,
       "width": 442
      }
     }
    }
   ],
   "source": [
    "a_init = np.random.rand(3,4)\n",
    "xcoord = np.linspace(0,2*np.pi,100)\n",
    "np.random.shuffle(xcoord)\n",
    "batchsize = 35\n",
    "subset = xcoord[0:batchsize]\n",
    "\n",
    "h = 0.001\n",
    "max_steps = 10000\n",
    "tolerance = 0.0001\n",
    "showInfo = False\n",
    "\n",
    "startFirst = time.time()\n",
    "algorithm1(h,max_steps,tolerance,a_init,showInfo)\n",
    "endFirst = time.time()\n",
    "\n",
    "startSecond = time.time()\n",
    "algorithm2(h,max_steps,tolerance,a_init,batchsize,subset,showInfo)\n",
    "endSecond = time.time()\n",
    "\n",
    "startThird = time.time()\n",
    "algorithm3(h,max_steps,tolerance,a_init,showInfo)\n",
    "endThird = time.time()\n",
    "\n",
    "startFourth = time.time()\n",
    "algorithm4(h,max_steps,tolerance,a_init,batchsize,subset,showInfo)\n",
    "endFourth = time.time()\n",
    "\n",
    "print(\"Using gradient descent, the algorithm took:\", endFirst - startFirst, \" seconds.\")\n",
    "print(\"Using stochastic gradient descent, the algorithm took:\", endSecond - startSecond, \" seconds.\")\n",
    "print(\"Using conjugate gradient descent, the algorithm took:\", endThird - startThird, \" seconds.\")\n",
    "print(\"Using stochastic conjugate gradient descent, the algorithm took:\", endFourth - startFourth, \" seconds.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Conclusion:\n",
    "\n",
    "In all the randomized trial cases, we see that using a stochastic approach (with and without conjugate) created an approximation faster than if it were to use all $x$ values from $0$ to $2\\pi$. On the other hand, using a conjugate approach did not create a noticeable improvement in speed compared to if the algorithm was run without a conjugate approach. Looking at the generated graphs gives a different interpretation of the algorithms. The algorithm that best approximated the $\\sin$ function was conjugate gradient descent. The flaws of the two conjugacy algorithms is that it tends to not approximate well at the endpoints of the range given. Nonetheless, it creates a better approximation to the $\\sin$ function than the gradient descent and stochastic gradient descent which both show huge variations at certain intervals. In fact, using both conjugate and stochastic approaches at the same time can create drastic problems in rare cases, as was seen in one simulation done (result shown here).\n",
    "\n",
    "<center><img src=\"disaster typo fixed.gif\"></center>\n",
    "\n",
    "\n",
    "To conclude, if time is a priority and you don't care so much for accuracy, use the stochastic gradient descent approach whereas if time is no problem for you, using the conjugate gradient descent approach will give better approximations. But it is worth noting that out of all four algorithms, the SCGD algorithm was able to best minimize the distances between the $\\sin$ function and the neural network constants, creating a cost of less than $1$ in less than $5$ seconds. An improvement that can be made to the algorithms is having less steps taken so as to not have the user get impatient. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Anaconda)",
   "language": "python",
   "name": "anaconda3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}